{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent function mentioned in the question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        #calculate gradient and the change in value of paramters (denoted by delta)\n",
    "        delta = -learn_rate*gradient(x)\n",
    "        #if the tolerance of all paramters is within tol, do not need to complete all the iterations\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        #update parameters\n",
    "        x+=delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a (i)\n",
    "Gradient descent on $x^2+3x+4$. The derivative of $x^2+3x+4$ is $2x+3$, which is passed as arguement of gradient desecent function. Learn rate set to 0.02 and 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minima is at -1.5 when the initialised value is 1\n",
      "The minima is at -1.5 when the initialised value is -1\n",
      "The minima is at -1.5 when the initialised value is 100\n",
      "The minima is at -1.5 when the initialised value is 1000\n",
      "The minima is at -1.5 when the initialised value is -100\n",
      "The minima is at -1.5 when the initialised value is -1000\n"
     ]
    }
   ],
   "source": [
    "initial_values = [1,-1,100,1000,-100,-1000]\n",
    "for val in initial_values:\n",
    "    argmin1 = gradient_descent(gradient=lambda v: 2 * v + 3, init_=val, n_iter = 1000, learn_rate=0.02)\n",
    "    print(\"The minima is at \"+ str(argmin1)+ \" when the initialised value is \"+str(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Some random initalisations give the same minima. The function is very simple quadratic function $ax^2+bx+c$ with one optimum at $\\frac{-b}{2a}$ and our findings are consistent with this.\n",
    "\n",
    "Final answer : Minima is at x = -1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a (ii)\n",
    "Gradient descent on $x^4-3x^2+2x$. The derivative of $x^4-3x^2+2x$ is $4x^3-6x+2$, which is passed as arguement of gradient desecent function. Learn rate set to 0.002 (a small learn rate because the derivatives are large and we do not want huge steps) and 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJzt3XmYVNWd//H3l0UWJQGkQQQBdYiAioCdDmoAZRM1IzrjGhfGOGF8/CVhJosanUSN0ScmM2MySTTBYELc4hq3iLIIqBBZBQWBgIraAQEXkEX28/vjWz202G1Xd1fVqXvr83qeem5VdTX3W6CfOnXuWSyEgIiIJF+T2AWIiEhuKNBFRFJCgS4ikhIKdBGRlFCgi4ikhAJdRCQlFOgiIimhQBcRSQkFuohISjQr5Mk6dOgQevToUchTiogk3oIFC94LIZTV9bqCBnqPHj2YP39+IU8pIpJ4ZvZWNq9Tl4uISEoo0EVEUkKBLiKSEgp0EZGUUKCLiKREVoFuZm3N7GEzW25my8zsBDNrb2ZTzGxl5tgu38WKiEjtsm2h/wJ4JoTQCzgOWAZcA0wLIfQEpmUei4hIJHUGupl9DhgMTAAIIewMIWwERgMTMy+bCJyVryJFRBJr+XK4/npYuzbvp8qmhX4EsAH4vZm9bGa/M7MDgU4hhLUAmWPHmn7ZzMaa2Xwzm79hw4acFS4ikggzZsCPfgQ7d+b9VNkEejNgAHBHCKE/sJV6dK+EEMaHEMpDCOVlZXXOXBURSZfFi6FtW+jWLe+nyibQK4HKEMKczOOH8YBfZ2adATLH9fkpUUQkwRYtgn79wCzvp6oz0EMI7wLvmNlRmaeGAa8BTwBjMs+NAR7PS4UiIkm1Zw+88gocd1xBTpft4lzfBO41swOAN4DL8A+DB83scuBt4Nz8lCgiklCrVsG2bd5CL4CsAj2EsAgor+FHw3JbjohIiixe7McCBbpmioqI5MuiRdCsGfTuXZDTKdBFRPJl0SLo0wdatCjI6RToIiL5snhxwbpbQIEuIpIf69fDmjUFG+ECCnQRkfwo8AVRUKCLiOTHokV+VAtdRCThFi+Grl3h4IMLdkoFuohIPlRN+S8gBbqISK5t3+7L5irQRUQS7pVXfB0XBbqISMItWODH448v6GkV6CIiubZgAbRvD927F/S0CnQRkVxbsMBb5wVYA706BbqISC5t3w5LlhS8uwUU6CIiufXqq7B7twJdRCTxIl0QBQW6iEhuLVgA7dpBjx4FP7UCXUQklxYujHJBFBToIiK5s2OH96FH6G4BBbqISO4sWQK7dinQRUQSL+IFUYBm2bzIzFYDm4E9wO4QQrmZtQceAHoAq4HzQggf5qdMEZEEqLogevjhUU5fnxb6KSGEfiGE8szja4BpIYSewLTMYxGR0rVgAQwYEOWCKDSuy2U0MDFzfyJwVuPLERFJqJ07o14QhewDPQCTzWyBmY3NPNcphLAWIHPsmI8CRUQS4dVXPdQjBnpWfejASSGENWbWEZhiZsuzPUHmA2AsQLdu3RpQoohIAsyZ48cvfSlaCVm10EMIazLH9cCfgQpgnZl1Bsgc19fyu+NDCOUhhPKysrLcVC0iUmzmzoWOHSFiw7XOQDezA82sTdV9YCSwBHgCGJN52Rjg8XwVKSJS9ObM8dZ5pAuikF2XSyfgz+ZFNgPuCyE8Y2bzgAfN7HLgbeDc/JUpIlLENm3yPUQvuihqGXUGegjhDeC4Gp5/HxiWj6JqtGcPNG1asNOJiGRt3jw/Ruw/h6TMFD3/fDj11NhViIjUrOqC6Be/GLWMZAR6+/b+Cbh3b+xKREQ+be5cOOooaNs2ahnJCPQBA+Cjj+DNN2NXIiLySSHsuyAaWXICHXydYRGRYvLOO7BuHVRUxK4kIYF+zDHQvLkCXUSKTxFMKKqSjEBv0cJDXYEuIsVm7lzPqL59Y1eSkEAH73ZZuND7q0REisWcOdC/PxxwQOxKEhbo770HlZWxKxERcbt3+5K5RdB/DkkLdFC3i4gUjyVLYNu2oug/hyQFet++0KSJAl1Eisfs2X488cS4dWQkJ9Bbt4bevRXoIlI8Zs2Czp2he/fYlQBJCnTwbpeqTVhFRGKbPRtOOinqCovVJS/Q1671m4hITGvWwOrVHuhFIlmBXrW108svx61DRGTWLD8WSf85JC3Q+/Xzo/rRRSS22bOhVSsfg14kkhXobdrAF76gQBeR+GbN8vHnzZvHruT/JCvQwfvR58+PXYWIlLJt27zrt4i6WyCJgV5R4aubvftu7EpEpFTNm+ezRIvogigkMdCrdgSp2vJJRKTQqi6InnBC3Dr2k7xA79/f9xadOzd2JSJSqmbP9omO7dvHruQTkhfoBx7oS+kq0EUkhr17900oKjLJC3TwfvR587SUrogU3vLl8OGHyQ50M2tqZi+b2VOZx4eb2RwzW2lmD5hZ4RYDrqjwv9DXXy/YKUVEAHj+eT9++ctx66hBfVro44Bl1R7fCtwWQugJfAhcnsvCPlPVhVF1u4hIoc2cCV26wJFHxq7kU7IKdDPrCpwB/C7z2IChwMOZl0wEzspHgTU6+mifoaVAF5FCCsEDfciQolmQq7psW+g/B64C9mYeHwxsDCHszjyuBLrU9ItmNtbM5pvZ/A0bNjSq2P/TrJmv66JAF5FCWrnSFwccMiR2JTWqM9DN7CvA+hBC9XVra/poqvEKZQhhfAihPIRQXlZW1sAya1BR4UsA7NqVuz9TROSzzJzpx6QGOnAScKaZrQb+hHe1/Bxoa2bNMq/pCqzJS4W1qaiAHTvg1VcLeloRKWEzZ0KnTr6mVBGqM9BDCN8PIXQNIfQALgCeCyFcBEwHzsm8bAzweN6qrEnVpqzqdhGRQijy/nNo3Dj0q4Fvm9kqvE99Qm5KylKPHnDwwQp0ESmMN9+Eysqi7W4BaFb3S/YJIcwAZmTuvwFU5L6kLJl5K33OnGgliEgJmTHDj0Uc6MmcKVpl4EBYtgw2boxdiYik3cyZ0KED9OkTu5JaJTvQTzzR+7XUSheRfCvy/nNIeqBXVECTJr5QjohIvrz1lt+KuLsFkh7on/scHHusAl1E8mv6dD+efHLUMuqS7EAH73aZMwf27IldiYik1dSp0LGjL91dxNIR6Js3w9KlsSsRkTQKwQN9+PCi7j+HtAQ6qNtFRPJj6VJYt84DvcglP9APP9yn4irQRSQfpkzx47BhcevIQvID3cxb6Qp0EcmHqVN97ZZu3WJXUqfkBzp4oL/+un8tEhHJlZ07ffx5ArpbIE2BDvDXv8atQ0TSZc4c2LpVgV5QAwbAAQeo20VEcmvqVJ+8eMopsSvJSjoCvWVL38FIgS4iuTR1qu9h3LZt7Eqyko5AB+92mTcPtm+PXYmIpMFHH3mXS0K6WyBNgT54sF/A0ProIpILM2b4DPQEDFeskp5AHzTIhzBW7fknItIYkybBQQftG3SRAOkJ9HbtfKGu55+PXYmIJF0IHujDh0OLFrGryVp6Ah18acvZs2HXrtiViEiSLVvmy+WedlrsSuolfYG+bRssWBC7EhFJskmT/KhAj2jQID+qH11EGuPpp32p3MMOi11JvaQr0Dt2hN691Y8uIg23eTO88AKcfnrsSuqtzkA3s5ZmNtfMFpvZUjO7MfP84WY2x8xWmtkDZnZA/svNwuDB8OKL2vBCRBpm2jS/DpfGQAd2AENDCMcB/YBRZjYQuBW4LYTQE/gQuDx/ZdbDkCE+IWDRotiViEgSTZrk21smaLhilToDPbgtmYfNM7cADAUezjw/ETgrLxXW1+DBflS3i4jUVwjefz5iBDRvHruaesuqD93MmprZImA9MAV4HdgYQtideUkl0CU/JdZTly5w5JG6MCoi9bd0KVRWJm50S5WsAj2EsCeE0A/oClQAvWt6WU2/a2ZjzWy+mc3fsGFDwyutj8GD/aLG3r2FOZ+IpMOTT/oxzYFeJYSwEZgBDATamlmzzI+6Amtq+Z3xIYTyEEJ5WVlZY2rN3tCh8MEHsHhxYc4nIunw+ONQUQGHHhq7kgbJZpRLmZm1zdxvBQwHlgHTgXMyLxsDPJ6vIutt6FA/Tp0atw4RSY41a3x1xdGjY1fSYNm00DsD083sFWAeMCWE8BRwNfBtM1sFHAxMyF+Z9XToodCnjw8/EhHJxhNP+PGs4hjf0RDN6npBCOEVoH8Nz7+B96cXp+HD4c47YceORC2uIyKRPPYY9OzpkxMTKl0zRasbNgw+/lj7jIpI3T76CJ57zlvnZrGrabD0BvqQIdC0qbpdRKRukyb57NAE959DmgP985/3q9W6MCoidXnsMV8LauDA2JU0SnoDHbzbZe5c2LQpdiUiUqx27vTZoWee6d/qEyzdgT58uE8u0qxREanNjBneh57w7hZIe6APHAitWqnbRURq99BDvndogjaDrk26A71FC18GQBdGRaQmu3bBo49667xVq9jVNFq6Ax282+W113zBHRGR6p57zpcJOe+82JXkRPoDvWqRnao9AkVEqjz4oK99PnJk7EpyIv2B3qeP7wuoQBeR6nbu3Nfd0rJl7GpyIv2Bbuat9KlT/R9QRAQ8EzZuhPPPj11JzqQ/0MH3Bty8GWbNil2JiBSLBx/0CYgjRsSuJGdKI9CHDvXtpNTtIiLgi/Y99hicfTYcUBz72+dCaQR6mzYwaJACXUTc00/7DPKUjG6pUhqBDt7tsmQJvPNO7EpEJLa774ZOnVLV3QKlFOgavigi4OPOn3oKLrwQmtW5JUSilE6g9+4N3bop0EVK3YMP+gzRSy+NXUnOlU6gm3m3y9SpfkFERErT3XfD0UdDv36xK8m50gl0gDPOgC1bfHU1ESk9r78Os2fDJZckemei2pRWoA8bBgceCI8/HrsSEYnhnns8yC+6KHYleVFagd6qFYwa5YG+d2/sakSkkELw7pZTToGuXWNXkxelFejg6zasWQPz58euREQKacYM73K57LLYleRNnYFuZoeZ2XQzW2ZmS81sXOb59mY2xcxWZo7t8l9uDpxxhm8zpW4XkdJy553Qti388z/HriRvsmmh7wa+E0LoDQwE/p+Z9QGuAaaFEHoC0zKPi1/79jBkiE/7FZHS8P778MgjcPHFqdjIojZ1BnoIYW0IYWHm/mZgGdAFGA1MzLxsInBWvorMudGjfdOLlStjVyIihXDPPb7a6te/HruSvKpXH7qZ9QD6A3OATiGEteChD3Ss5XfGmtl8M5u/YcOGxlWbK1WbwarbRST9QvDulooK6Ns3djV5lXWgm9lBwCPAv4cQPsr290II40MI5SGE8rKysobUmHvdu0P//up2ESkFL70ES5emvnUOWQa6mTXHw/zeEMKjmafXmVnnzM87A+vzU2KejB7tEwzefTd2JSKST+PH+/yTFG1kUZtsRrkYMAFYFkL4n2o/egIYk7k/BkhW/8U55/hXsUcfrfu1IpJM770H99/vM0PbtIldTd5l00I/CbgEGGpmizK304GfACPMbCUwIvM4OY4+2vcbfeCB2JWISL7ceaev3fSNb8SupCDqXDsyhPAiUNuiB8NyW06BnX8+3HCDTzQ69NDY1YhILu3eDXfc4Ut+HH107GoKovRmilZ33nne7fLww7ErEZFce/xx39Dmm9+MXUnBlHag9+rlw5jU7SKSPr/8JfToAV/5SuxKCqa0Ax2822X2bG1NJ5Imr7wCM2fClVf6Uh8lQoFetUnsQw/FrUNEcue//xtat4bLL49dSUEp0P/hH2DAAPjTn2JXIiK58PbbcN99PpGoffvY1RSUAh3gq1+FefNgxYrYlYhIY912mw92+Pa3Y1dScAp08EBv0sQX8BGR5PrgAx97fuGFvil8iVGgA3TuDCNG+G4m2slIJLluvx22boWrropdSRQK9CqXXgpvvQUvvBC7EhFpiG3b4H//F04/HY49NnY1USjQq5x1Fhx0EPzxj7ErEZGG+M1vYMMGuCYZe+3kgwK9SuvWvmDXQw/5J72IJMfWrfCTn/g0/0GDYlcTjQK9uksvhc2b4YknYlciIvVx++3eOr/xxtiVRKVAr27IEL8yPmFC7EpEJFtbtsBPfwojR8JJJ8WuJioFenVNmvjMsqlT4Y03YlcjItn41a983fMSb52DAv3TvvY1D/bf/S52JSJSlw8/9Nb5aafBwIGxq4lOgb6/rl3hjDPgrrtg167Y1YjIZ7n5Zti40S+IigK9RmPHwrp18OSTsSsRkdq8+aYvkfsv/+LLYIsCvUajRnlLffz42JWISG2uvdaXxr3pptiVFA0Fek2aNfOLo5MneytARIrL3Lm+Qup3vwtdusSupmgo0Gtz+eV+cfSOO2JXIiLV7d0L48ZBp07wve/FrqaoKNBrc9hhcPbZvnLb1q2xqxGRKnfdBS+95KNb2rSJXU1RqTPQzewuM1tvZkuqPdfezKaY2crMsV1+y4xk3Di/gn733bErERHw8eZXX+3T+y+5JHY1RSebFvofgFH7PXcNMC2E0BOYlnmcPiedBMcf7yu4hRC7GhH5/vdh0yaf6m8Wu5qiU2eghxCeBz7Y7+nRwMTM/YnAWTmuqziYeSt92TKYMiV2NSKlbfZsn/D3H/8BxxwTu5qi1NA+9E4hhLUAmWPH3JVUZM47zy++/OIXsSsRKV0ffwyXXeZrLf3wh7GrKVp5vyhqZmPNbL6Zzd+wYUO+T5d7LVrAlVfC00/Da6/FrkakNP3nf8Lf/uYXRHUhtFYNDfR1ZtYZIHNcX9sLQwjjQwjlIYTysrKyBp4usiuv9PXSb701diUipWfWLN/4+YorfL1zqVVDA/0JYEzm/hjg8dyUU6Q6dIB/+ze4915YvTp2NSKlY+vWfV0tP/1p7GqKXjbDFu8H/gocZWaVZnY58BNghJmtBEZkHqfbd77jE41+9rPYlYiUjm9+E1atgt//Xl0tWWhW1wtCCBfW8qPS+u7TpQuMGeObX/zgB3DIIbErEkm3e+7xIP/BD+CUU2JXkwiaKVofV13lS+redlvsSkTS7W9/8z7zQYM0qqUeFOj10bOnD2P89a9hfa3XgUWkMbZsgXPPhZYt4b77fLE8yYoCvb5uuMHHxGpBfZHc27vX1zdfssS7XLp2jV1RoijQ6+uoo7wv/fbbobIydjUi6XLTTfDIIz74YNT+K45IXRToDXH99d6S0ML6Irnz0EP+DXjMGJ/eL/WmQG+I7t19XPqECT6kSkQaZ/p0uPhiOPFE+M1vtPBWAynQG+q663xZgGuvjV2JSLItXAijR/uggyef9Iuh0iAK9IY65BAfxvjQQ/D887GrEUmmFSvgtNOgXTt49llo3z52RYmmQG+M733PdzYaNw727IldjUiyLF0KQ4b4/cmTtTdoDijQG6N1a19fYtEin9EmItlZvBhOPtmX05g500ePSaMp0Bvr/PN9Z6Nrr/WdVETks73wgk/lb9nSw7xXr9gVpYYCvbHMfPOL997TBVKRutx3HwwfDh07+rWnnj1jV5QqCvRcOP54XxXujjt8mywR+aSqeRsXXQQDB/r/J4cfHruq1FGg58qPf+wXSP/1X2HHjtjViBSP99+Hf/xHX2Trkkv8AqhGs+SFAj1X2rTxFvqyZVrnRaTK7NkwYABMneqL2k2c6PM3JC8U6Ll0+ulwwQVw880+8kWkVH38sQ/rHTTIR7K8+KJv5agZoHmlQM+1X/4SDj4YvvpV2LYtdjUihTdtGvTvD//1X/D1r8Mrr8AXvxi7qpKgQM+1Dh38a+WyZT6TVKRUvPEG/NM/+SiWnTthyhRfl0VbxxWMAj0fRo701eJ+/Wt46qnY1Yjk1+rVvlhdr15+wfOWW+C11zzYpaAU6Plyyy3Qt68v1r96dexqRHJv0SK4/HIfS/6HP/gIrxUr4Pvf1wJbkSjQ86VlS3j4Ydi927+Gfvxx7IpEGm/zZvjjH32Z2/79faLQFVfA66/7pi9ajyWqRgW6mY0ysxVmtsrMrslVUanRsyfce6+3ZMaOhRBiVyRSfx9+CPffD2efDWVlvgHF++/7Zulr1vhAAG0VVxQavPuqmTUFfg2MACqBeWb2RAjhtVwVlwpnnAE33uiTKvr1g+98J3ZFIp9t3Tpfo3zmTB+xsnChz/Q89FDvKz/3XF+/SEMQi05jttOuAFaFEN4AMLM/AaMBBfr+rrsOXn0Vvvtd/0p6wQWxK5LPEgJs3err81Td3n/fW6rbtn36tmuX/87+NzM44ACfSNOihXfDVd2vety6NbRq5bfq9/d/3Lq1/1m5CNEQvO533/WRKa+/7sflyz28//53f13z5j5N/4c/hBEj/H4T9dIWs8YEehfgnWqPK4EvNa6clGrSxPsd16+HSy/1oY0aARBPCN5VsHw5vPkmvP32J2+VlXUv31A9aJs396Dd/xaCD9/bscNv27f7ce/ehtVtVnvYV/8gaNXKz139nNu3w0cf7fuA2r79k392ixZw5JG+CuKAAb4+0YABcNBBDatVomhMoNfUVPhUJ7GZjQXGAnTr1q0Rp0u4li3hscdg8GDvi5w6Fb6kz7+8qgruBQv8G9Ly5ftuW7bse12TJt6d0K0bVFT4ReyyMv/grbodfLDvqnPggf5v2ZiW6u7dHqjbt/vF8qrbtm21P/6sn1U93rhx32OzT34raNnSvx3267fvPXXqBEcc4bdDD1XrOwUaE+iVwGHVHncF1uz/ohDCeGA8QHl5eWlfFWzbFiZN8l1aRoyAZ57x0QKSG5WVMHeudxssWODH9ev3/bxrVx8rfdllfuzVy8OsSxdvZRdKs2be8lXrV3KsMYE+D+hpZocDfwcuAL6ak6rSrEsXv9g0dCiceir85S/eapf62bvXJ6+8+OK+21tv+c+aNoU+fXxtnQED/Na3r2YsSuo1ONBDCLvN7BvAs0BT4K4QwtKcVZZmXbrAjBkwbBiMGgX33ONf86V2IfiklWee8e6qWbO8iwG862DQIJ+dO3Cgh3erVnHrFYmgMS10QghPA0/nqJbS0rmzh/ro0XDOOXDrrT4KRkPB9tm0yYfNPfusB/nbb/vzX/iC/519+ct+O+II/b2J0MhAl0bq2BGee877dK+6yrsQfvUrv/BWivbuhZdf9vB+9llfS3vPHu8qGTbMt/g79VTo0SN2pSJFSYEeW6tWPn36qKN8i645c+CBB+DYY2NXVhjr1/uCTs8848cNG/z5AQP8Q27UKDjhhMJetBRJKAV6MWjSxGeTDhoEF1/sQ+duucX3KW2Wsn+iXbvgr3/d142ycKE/36GDt75PPdVXq+zUKW6dIglkoYDri5SXl4f58+cX7HyJtG6dr2D3l794K/W3v4Xy8thVNc7q1R7gzz7rFzQ3b/aRKCec4C3wUaN8oSeNgxapkZktCCHUGQQpa/6lQKdO8OSTvlLjt77lrfVLLoEbbkjOLumbN/sF38mTfZODFSv8+cMO82UPRo3yPvHPfz5qmSJpoxZ6Mdu0CX78Y79QumcPfO1rMG4c9O4du7JP2rPHu04mT/bb7Nk+G7JVK59ENXKkh3ivXhqNItIA2bbQFehJsGaNB/uECb42yIgRvurd6afHGW+9YwfMm+eTeV54wceEb9rkPxswwOsbOdJX5NMO7yKNpkBPo/Xr4c474Y47fEW8gw6CM8/0JXqHDoVDDsn9OXfv9i6ThQt9SOG8eX6rWryqd28fC37yyb7gWMeOua9BpMQp0NNs927vo37gAXj0UfjgA3++V69909yPOQa6d/dFl9q1q72rIwTv8163zm9vvgkrV+67LV26b2W+li3huOO85T1okB/LygrylkVKmQK9VOzZ4y3n6dPh+edh8WJ4551PvqZFC2/Nt2q1b6/HqtX+tmz59FKqZv5h0LMnHH30vvVQjjoqfcMoRRJAgV7KPvzQZ51WVnrXzNq1vmFD1XKtZh7sLVv6rNSOHX10TadOvoTsEUeo71ukiGjYYilr1867Q0SkpGgmh4hISijQRURSQoEuIpISCnQRkZRQoIuIpIQCXUQkJRToIiIpoUAXEUmJgs4UNbMNwFsN/PUOwHs5LCemtLyXtLwP0HspVml5L419H91DCHUunFTQQG8MM5ufzdTXJEjLe0nL+wC9l2KVlvdSqPehLhcRkZRQoIuIpESSAn187AJyKC3vJS3vA/ReilVa3ktB3kdi+tBFROSzJamFLiIinyFRgW5mN5nZK2a2yMwmm9mhsWtqKDP7mZktz7yfP5tZ29g1NYSZnWtmS81sr5klcjSCmY0ysxVmtsrMroldT0OZ2V1mtt7MlsSupTHM7DAzm25myzL/bY2LXVNDmVlLM5trZosz7+XGvJ4vSV0uZva5EMJHmfvfAvqEEK6IXFaDmNlI4LkQwm4zuxUghHB15LLqzcx6A3uB3wLfDSEkaksqM2sK/A0YAVQC84ALQwivRS2sAcxsMLAF+GMI4ZjY9TSUmXUGOocQFppZG2ABcFZC/00MODCEsMXMmgMvAuNCCC/l43yJaqFXhXnGgUByPo32E0KYHELYnXn4EtA1Zj0NFUJYFkJYEbuORqgAVoUQ3ggh7AT+BIyOXFODhBCeBz6IXUdjhRDWhhAWZu5vBpYBXeJW1TDBbck8bJ655S23EhXoAGZ2s5m9A1wE/DB2PTnyNWBS7CJKVBeg+q7alSQ0PNLIzHoA/YE5cStpODNramaLgPXAlBBC3t5L0QW6mU01syU13EYDhBCuCyEcBtwLfCNutZ+trveSec11wG78/RSlbN5HglkNzyX2m1+amNlBwCPAv+/37TxRQgh7Qgj98G/hFWaWt+6wotskOoQwPMuX3gf8Bbg+j+U0Sl3vxczGAF8BhoUivphRj3+TJKoEDqv2uCuwJlItkpHpb34EuDeE8GjsenIhhLDRzGYAo4C8XLguuhb6ZzGzntUengksj1VLY5nZKOBq4MwQwrbY9ZSweUBPMzvczA4ALgCeiFxTSctcSJwALAsh/E/sehrDzMqqRrCZWStgOHnMraSNcnkEOAofVfEWcEUI4e9xq2oYM1sFtADezzz1UhJH7JjZ2cAvgTJgI7AohHBq3Krqx8xOB34ONAXuCiHcHLmkBjGz+4GT8ZX91gHXhxAmRC2qAczsy8ALwKv6fTO8AAAAV0lEQVT4/+sA14YQno5XVcOYWV9gIv7fVhPgwRDCj/J2viQFuoiI1C5RXS4iIlI7BbqISEoo0EVEUkKBLiKSEgp0EZGUUKCLiKSEAl1EJCUU6CIiKfH/AeFms2bpnu8mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f448cadf550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-3,3,1000)\n",
    "\n",
    "# the function, which is y = x^2 here\n",
    "y = x*x*x*x - 3*x*x + 2*x\n",
    "\n",
    "# setting the axes at the centre\n",
    "fig = plt.figure()\n",
    "\n",
    "# plot the function\n",
    "plt.plot(x,y, 'r')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minima is at -1.366 when the initialised value is 0\n",
      "The minima is at 1.0 when the initialised value is 1\n",
      "The minima is at 1.0 when the initialised value is 2\n",
      "The minima is at 1.0 when the initialised value is 3\n",
      "The minima is at 1.0 when the initialised value is 4\n",
      "The minima is at 1.0 when the initialised value is 5\n",
      "The minima is at 1.0 when the initialised value is 6\n",
      "The minima is at 1.0 when the initialised value is 7\n",
      "The minima is at 1.0 when the initialised value is 8\n",
      "The minima is at 1.0 when the initialised value is 9\n"
     ]
    }
   ],
   "source": [
    "for value in range(10):\n",
    "    argmin2 = gradient_descent(gradient=lambda v: 4*v*v*v - 6*v + 2, init_=value, learn_rate=0.002,n_iter=1000)\n",
    "    print(\"The minima is at \"+ str(argmin2)+ \" when the initialised value is \"+str(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion : 0 gives another minima as compared to 1-9\n",
    "Check for x = 1. Derivative at x= 1 is 4-6+2 = 0 (one of the minimas). Need to check whether it is global or local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minima is at -1.366 when the initialised value is 0\n",
      "The minima is at -1.366 when the initialised value is -1\n",
      "The minima is at -1.366 when the initialised value is -2\n",
      "The minima is at -1.366 when the initialised value is -3\n",
      "The minima is at -1.366 when the initialised value is -4\n",
      "The minima is at -1.366 when the initialised value is -5\n",
      "The minima is at -1.366 when the initialised value is -6\n",
      "The minima is at -1.366 when the initialised value is -7\n",
      "The minima is at -1.366 when the initialised value is -8\n",
      "The minima is at -1.366 when the initialised value is -9\n"
     ]
    }
   ],
   "source": [
    "for value in range(10):\n",
    "    argmin2 = gradient_descent(gradient=lambda v: 4*v*v*v - 6*v + 2, init_=-value, learn_rate=0.002,n_iter=1000)\n",
    "    print(\"The minima is at \"+ str(argmin2)+ \" when the initialised value is \"+str(-value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-9 to 0 give the same minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x*x*x*x-3*x*x+2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.848076206064\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(func(-1.366))\n",
    "print(func(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.396\n"
     ]
    }
   ],
   "source": [
    "#Trying at a large negative value (the learn rate is kept very small because the derivative is very large)\n",
    "argmin2 = gradient_descent(gradient=lambda v: 4*v*v*v - 6*v + 2, init_=-500, learn_rate=0.000002,n_iter=2000000)\n",
    "print(argmin2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, there are 2 minimas:\n",
    "    1 global at x = -1.366,\n",
    "    1 local at x = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient(a,b,X,y):\n",
    "    n = len(y)\n",
    "    \n",
    "    #calculate precited values considering weights to be w\n",
    "    predicted = a*X+b\n",
    "    \n",
    "    #error in prediction\n",
    "    error = y-predicted\n",
    "    \n",
    "    gradient_a = sum(error*X)\n",
    "    gradient_b  = sum(error)\n",
    "    \n",
    "    #return average gradient\n",
    "    return (-2*gradient_a/n,-2*gradient_b/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,init_, learn_rate, n_iter=1000, tol=1e-06):\n",
    "    begin = time.time()\n",
    "    a = init_[0]\n",
    "    b = init_[1]\n",
    "    for _ in range(n_iter):\n",
    "        (delta_a,delta_b) = linear_regression_gradient(a,b,X,y)\n",
    "        delta_a*= -learn_rate\n",
    "        delta_b*= -learn_rate\n",
    "        if abs(delta_b)<=tol and abs(delta_a)<=tol:\n",
    "            break\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "    end = time.time()\n",
    "    print(end-begin)\n",
    "    return np.round_(np.array([a,b]), decimals = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = 2.5* np.random.randn(10000)+1.5\n",
    "res = 1.5* np.random.randn(10000)\n",
    "y = 2+0.3*X+res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4164843559265137\n",
      "[0.295 2.023]\n"
     ]
    }
   ],
   "source": [
    "ans = gradient_descent(X,y,init_=np.zeros(2), learn_rate=0.02)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient(a,b,X,y):\n",
    "    n = len(y)\n",
    "    \n",
    "    #calculate precited values considering weights to be w\n",
    "    predicted = a*X+b\n",
    "    \n",
    "    #error in prediction\n",
    "    error = y-predicted\n",
    "    \n",
    "    gradient_a = sum(error*X)\n",
    "    gradient_b  = sum(error)\n",
    "    \n",
    "    #return average gradient\n",
    "    return (-2*gradient_a/n,-2*gradient_b/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X,y,init_, learn_rate, n_iter=4000, tol=1e-06, batch_size = 30):\n",
    "    begin = time.time()\n",
    "    a = init_[0]\n",
    "    b = init_[1]\n",
    "    n = len(y)\n",
    "    for _ in range(n_iter):\n",
    "        random_indices = np.random.choice(n, size=batch_size, replace=False)\n",
    "        (delta_a,delta_b) = linear_regression_gradient(a,b,X[random_indices],y[random_indices])\n",
    "        delta_a*= -learn_rate\n",
    "        delta_b*= -learn_rate\n",
    "        if abs(delta_b)<=tol and abs(delta_a)<=tol:\n",
    "            break\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "    end = time.time()\n",
    "    print(end-begin)\n",
    "    return np.round_(np.array([a,b]), decimals = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7095179557800293\n",
      "[0.31  2.039]\n"
     ]
    }
   ],
   "source": [
    "ans = batch_gradient_descent(X,y,init_=np.zeros(2), learn_rate=0.02)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.725426435470581\n",
      "Batch size: 10 optimum weights: 0.305 and 2.005\n",
      "2.077805519104004\n",
      "Batch size: 30 optimum weights: 0.279 and 2.014\n",
      "1.8384864330291748\n",
      "Batch size: 50 optimum weights: 0.296 and 2.035\n",
      "2.1734752655029297\n",
      "Batch size: 100 optimum weights: 0.314 and 2.01\n",
      "3.2683160305023193\n",
      "Batch size: 500 optimum weights: 0.299 and 2.028\n",
      "4.4966301918029785\n",
      "Batch size: 1000 optimum weights: 0.313 and 2.024\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [10,30,50,100,500,1000]\n",
    "for size in batch_sizes:\n",
    "    ans = batch_gradient_descent(X,y,init_=np.zeros(2), learn_rate=0.02, batch_size=size)\n",
    "    print(\"Batch size: \"+str(size)+\" optimum weights: \"+ str(ans[0]) + \" and \" + str(ans[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
