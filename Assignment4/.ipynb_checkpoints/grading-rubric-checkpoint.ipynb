{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End sem grading rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T/F answers (for reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. N\n",
    "2. 10110\n",
    "3. $N(w|w_0,\\sigma^2 I)$\n",
    "4. O(D), O(N)\n",
    "5. 1NN< DT < LR < kSVM, DT < LR < kSVM < 1NN\n",
    "6. $\\pi_k = \\frac{1}{K} \\forall k\\in \\{1 \\cdots K\\}$, $\\Sigma_k = \\sigma^2 \\mathcal{I} \\forall k\\in \\{1 \\cdots K\\}$\n",
    "7. $0.5 Bern(x|\\pi_1) + 0.5 Bern(x|\\pi_2)$\n",
    "8. $O(D^2)$, $\\infty$\n",
    "\n",
    "For Qs 5 and 6, give 2 marks for each correct blank filled. For all questions, if answers are not correct, but show some glimmers of understanding, give half marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter estimation\n",
    "\n",
    "(a) Poisson rate parameter MLE\n",
    "\n",
    "- give +1 mark for correctly calculating the likelihood function given n data samples $x_1, \\cdots x_n$ as a product of n Poissons\n",
    "- give +2 mark for correctly reducing to log form\n",
    "- give +2 mark for correctly calculating the derivative w.r.t. $\\lambda$\n",
    "- give +1 mark for getting to the right answer, viz. the MLE for $\\lambda$ is the sample mean $\\frac{1}{n}\\sum_{i=1}^n{x_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Dating disasters\n",
    "\n",
    "- give +2 marks for correctly realizing that the likelihood is a product of Bernoulli trials $p^2(1-p)^6$ and the prior is Beta(2,2), which effectively reduces to $p(1-p)$ \n",
    "- give +2 marks for correctly calculating the MLE using the likelihood by first taking the log and then differentiating to get 0.25. Give zero for only writing the answer.\n",
    "- give +2 marks for correctly calculating the MAP using the posterior by multiplying the likelihood with the prior, then taking the log and then differentiating to get 0.3. Give zero for only writing the answer. \n",
    "- give +2 marks for correctly specifying that the prior is updated to Beta(4,8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) MLE anomaly\n",
    "\n",
    "- give +1 point for correctly specifying the log likelihood as $L(\\mu) = -\\frac{1}{2}(x - \\mu)^2 - \\frac{1}{2}\\log(2\\pi)$\n",
    "- give +1 point for calculating MLE for $\\mu$ as $x$.\n",
    "- give +1 point for realizing that this can only remain true when $x\\in [a,b]$\n",
    "- give +1 point for any progress at all towards realizing that the MLE calculated analytically is incomplete.\n",
    "- give +2 points for getting to the right answer that $\\mu = x$ when x is between a and b, is equal to a when $x<a$ and b when $x>b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Tweaks to classifiers\n",
    "\n",
    "- Give +1 for mentioning the need to discretize real x,y coordinates for DT to work\n",
    "- Give +1 for mentioning multinomial logistic regression \n",
    "- Give +1 for mentioning either pairwise or one-vs-all for SVM\n",
    "- Give +1 for saying the 1-NN will work fine without changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)-(c) for both these parts, look at the image below and mark answers using the associated rubric. \n",
    "\n",
    "\n",
    "![image info](./classifiers_outputs.png)\n",
    "\n",
    "For (b)\n",
    "\n",
    "- Give +1 for getting the DT boundaries right\n",
    "- Give +1 for getting the LR boundaries right\n",
    "- Give +2 for getting the SVM boundaries right (I am showing boundaries for pairwise training, one vs all will be different. make sure peoples' boundaries match what they wrote in (a))\n",
    "- Give +2 for getting 1NN boundaries right\n",
    "\n",
    "For (c)\n",
    "\n",
    "- Give -1 for each point missed in any of the four graphics (obviously capping the cuts at 6 points)\n",
    "\n",
    "For (d)\n",
    "\n",
    "- Give +2 for reporting the multi-class confusion matrix itself or the multi-class F1 score in a one-vs-all comparison. \n",
    "- Give +1 for pointing out that the F score ignores true negatives\n",
    "- Give +1 for pointing out that one-vs-rest F score is spuriously inflated in settings with few positive and many negative examples because it ignores true negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMs and optimization\n",
    "\n",
    "(a) Finding the SVM margin\n",
    "\n",
    "- give +1 point for saying that the SVM decision boundary tries to maximize the margin between the two classes\n",
    "- give +1 point for realizing that the points halfway between the support vectors on both sides are (3,4) and (2,5) and realizing that this means the decision boundary is x+y = 7, so $w_1 = w_2$ \n",
    "- give +1 point for plugging support vector coordinates into the hyperplane equation to get an equation pair like $2w_1 + 3w_2 + b = 1$ and $4w_1 + 5w_2 + b = -1$\n",
    "- give +1 point for solving the system of three equations for $\\{w_1, w_2, b\\}$. I get them as $\\{-0.5, -0.5, 3.5\\}$ \n",
    "- give +1 point for saying there are 5 support vectors in the dataset\n",
    "- give +1 point for calculating the margin as $2/\\|w\\| = 2\\sqrt(2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Proving NN square loss non-convexity. Here is one possible approach.\n",
    "\n",
    "- Consider a path in a simple 3 layer NN, where an input node connects to a hidden node through weight $w_2$ and the hidden node connects to the output through weight $w_1$.\n",
    "- Give input $x$ and output $y$, squared error loss here would be $L({\\bf w}) = (y - w_2 w_1 x)^2$. Give +3 for being able to state this much. \n",
    "- We have to show that $L({\\bf w})$ is non-convex in ${\\bf w}$. We can do this by showing violations of Jensen's inequality for some values of ${\\bf w}$. Give +2 for proposing a coherent strategy like this one for the proof. \n",
    "- Give +2 for applying Jensen's inequality (or whatever other result one is using to prove non-convexity) correctly. \n",
    "- Give +3 for successfully demonstrating a counter-example. I give one such demonstration below. \n",
    "\n",
    "Assume $x=1, {\\bf w} = [2, 1/2]^T$ and some ${\\bf u} = [-1, 1]^T$ so that if $f({\\bf w}; x) = w_2 w_1 x$ is convex, by Jensen's inequality, for any non-negative $\\alpha$, $$f(\\alpha {\\bf w} + (1 - \\alpha){\\bf u}) \\leq \\alpha f({\\bf w}) + (1 - \\alpha) f({\\bf u}).$$\n",
    "\n",
    "Further, define ${\\bf v} = \\alpha {\\bf w} + (1 - \\alpha){\\bf u}$, which equals $[3\\alpha - 1, 1 - \\alpha/2]^T$ for our choice of ${\\bf w}, {\\bf u}$.\n",
    "\n",
    "Plugging back into the Jensen's inequality, the LHS becomes $f({\\bf v}) = (3\\alpha - 1)(1 - \\alpha/2)$, while the RHS becomes $\\alpha(1) + (1 - \\alpha)(-1) = 2\\alpha - 1$. It can be easily shown that the LHS is only less than the RHS if $\\alpha>1$, so for all $\\alpha \\in [0, 1)$, we have shown a contradiction.\n",
    "\n",
    "Thus the function is non-convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Gradient descent variations\n",
    "\n",
    "- Give +1 for correctly identifying the difference between GD and SGD, viz. one point used for gradient calculation in SGD vs the whole dataset used in GD\n",
    "- Give +1 for correctly identifying computational savings as the advantage of SGD over GD\n",
    "- Give +1 for correctly identifying the difference between mini-batch SGD and SGD, viz. using a small batch of points instead of just one point for gradient calculations\n",
    "- Give +1 for correctly identifying stability of the gradient calculation as the advantage of minibatch SGD over SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM and clustering\n",
    "\n",
    "(a) Mixture of exponentials EM design\n",
    "\n",
    "- Give +1 for saying that we will run the E-step and M-step alternately until convergence\n",
    "- Give +3 for calculating the responsibility correctly in the E-step $$z_{nk} = \\frac{\\pi_k \\lambda_k e^{-\\lambda_k x_n}}{\\sum_{k=1}^K{\\pi_k \\lambda_k e^{-\\lambda_k x_n}}}$$\n",
    "- Give +3 for deriving the correct log likelihood to minimize in the M step $$ L = \\sum_{nk} z_{nk} (\\lambda_k x_n - \\log\\lambda_k - \\log\\pi_k)$$\n",
    "- Give +2 for adding a Lagrangian to handle the mixture weights normalization constraint $$ L = \\sum_{nk} z_{nk} (\\lambda_k x_n - \\log\\lambda_k - \\log\\pi_k) + \\gamma (\\sum_k \\pi_k - 1)$$\n",
    "- Give +3 for differentiating and calculating each of the parameter updates correctly. The correct updates are given below.\n",
    "\n",
    "$$\\pi_k = \\frac{\\sum_n{z_{nk}}}{\\sum_{nk}{z_{nk}}}$$\n",
    "$$\\gamma = \\sum_{nk}{z_{nk}}$$\n",
    "$$\\lambda_k^{-1} = \\frac{\\sum_n{z_{nk}x_n}}{\\sum_n{z_{nk}}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) k-means to PCA\n",
    "\n",
    "- Give +1 for saying that $z_{n}$ goes from a one-hot vector to a real vector.\n",
    "- Give +2 for saying that we add orthonormality constraints ${\\bf w}_i^T {\\bf w}_j = 0$ for $i \\neq j$ and $\\|{\\bf w}_i\\|^2 = 1$ to the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Reconstructing matrix from eigenvalues and eigenvectors\n",
    "\n",
    "- Give +1 for stating that eigendecomposition factorizes a matrix $M = ABA^{-1}$, where $A$ is a matrix made up of the eigenvectors, and $B$ is a diagonal matrix containing the eigenvalues.\n",
    "- Give +1 for correctly creating A =  (-1 1; 2 1) and B = (-1 0; 0 3).\n",
    "- Give +1 for correctly inverting A\n",
    "- Give +2 for putting the pieces together and calculating M = (5/3 4/3; 8/3 1/3). Being off by a scale factor makes no difference. Half marks if people don't complete the calculation."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e519745e6ba10d30c5b71754e8c36e17431f182e15aa66601967c314858ab17"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
